{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[8]ConvLSTM",
      "provenance": [],
      "authorship_tag": "ABX9TyNuLPoBsltQizmREX1uQfxw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doyeo-n/-Capstone-8_code/blob/main/%5B8%5DConvLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPrsv2Sldlb5"
      },
      "outputs": [],
      "source": [
        "#필요패키지 다운\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from impyute.imputation.cs import mice\n",
        "\n",
        "!pip install dask\n",
        "!pip install \"dask[dataframe]\"\n",
        "!pip install livelossplot\n",
        "!pip install keijzer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import dask.dataframe as dd\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout, CuDNNGRU, CuDNNLSTM\n",
        "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
        "from time import time\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from livelossplot import PlotLossesKeras\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
        "#from keras.utils.training_utils import multi_gpu_model\n",
        "from tensorflow.python.client import device_lib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.layers import InputLayer, ConvLSTM2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "#from keijzer import *\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"} # Make sure the axis background of plots is white, this is usefull for the black theme in JupyterLab\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_lstm_format(df, test_size=0.3, look_back=5, target_column='target', scale_X=True):\n",
        "    \"\"\"\n",
        "    TODO: output train and test datetime\n",
        "    Input is a Pandas DataFrame. \n",
        "    Output is a np array in the format of (samples, timesteps, features).\n",
        "    Currently this function only accepts one target variable.\n",
        "    Usage example:\n",
        "    # variables\n",
        "    df = data # should be a pandas dataframe\n",
        "    test_size = 0.5 # percentage to use for training\n",
        "    target_column = 'c' # target column name, all other columns are taken as features\n",
        "    scale_X = False\n",
        "    look_back = 5 # Amount of previous X values to look at when predicting the current y value\n",
        "    \"\"\"\n",
        "    from array import array\n",
        "    df = df.set_index(index_col)\n",
        "    sha = df.shape\n",
        "    a = round(sha[0]*0.3)\n",
        "    b = len(df[:-a])\n",
        "    c = b%7\n",
        "    d = len(df[-a : -6])\n",
        "    e = d%7\n",
        "    print(a, b, c, d, e)\n",
        "    train, test = df[c:-a], df[-a+e:-6] \n",
        "    print(train.shape, test.shape)\n",
        "    train =  np.array(train)\n",
        "    test = np.array(test)\n",
        "\n",
        "    # ...train\n",
        "    X_train, y_train = train[:,:-1], train[:,-1]\n",
        "    X_test, y_test = test[:,:-1], test[:,-1]\n",
        "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "    # Scale the features\n",
        "\n",
        "    if scale_X:\n",
        "        scalerX = StandardScaler(with_mean=True, with_std=True).fit(X_train)\n",
        "        X_train = scalerX.transform(X_train)\n",
        "        X_test = scalerX.transform(X_test)\n",
        "  \n",
        "    # Reshape the arrays\n",
        "    target_location = 9\n",
        "    samples = len(X_train) # in this case 217 samples in the training set\n",
        "    num_features = target_location # All columns before the target column are features\n",
        "\n",
        "    samples_train = X_train.shape[0] - look_back\n",
        "    X_train_reshaped = np.zeros((samples_train, look_back, num_features))\n",
        "    y_train_reshaped = np.zeros((samples_train))\n",
        "\n",
        "    for i in range(samples_train):\n",
        "        y_position = i + look_back\n",
        "        X_train_reshaped[i] = X_train[i:y_position]\n",
        "        y_train_reshaped[i] = y_train[y_position]\n",
        "\n",
        "\n",
        "    samples_test = X_test.shape[0] - look_back\n",
        "    X_test_reshaped = np.zeros((samples_test, look_back, num_features))\n",
        "    y_test_reshaped = np.zeros((samples_test))\n",
        "\n",
        "    for i in range(samples_test):\n",
        "        y_position = i + look_back\n",
        "        X_test_reshaped[i] = X_test[i:y_position]\n",
        "        y_test_reshaped[i] = y_test[y_position]\n",
        "    \n",
        "    return X_train_reshaped, y_train_reshaped, X_test_reshaped, y_test_reshaped\n"
      ],
      "metadata": {
        "id": "bIprmQVndn3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split(df):\n",
        "  df = df.set_index(index_col)\n",
        "  \n",
        "  X_train, y_train, X_test, y_test = df_to_lstm_format(df=df, test_size=test_size, look_back=look_back, target_column='log_cyan', scale_X=True)\n",
        "  X_train.shape\n",
        "\n",
        "  X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1, X_train.shape[2], 1)\n",
        "  X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1, X_test.shape[2], 1)\n",
        "  X_train.shape\n",
        "  return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    import keras.backend as K\n",
        "    \"\"\"\n",
        "    Returns the mean absolute percentage error.\n",
        "    For examples on losses see:\n",
        "    https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
        "    \"\"\"\n",
        "    return (K.abs(y_true - y_pred) / K.abs(y_pred)) * 100\n",
        "\n",
        "def smape(y_true, y_pred):\n",
        "    import keras.backend as K\n",
        "    \"\"\"\n",
        "    Returns the Symmetric mean absolute percentage error.\n",
        "    For examples on losses see:\n",
        "    https://github.com/keras-team/keras/blob/master/keras/losses.py\n",
        "    \"\"\"\n",
        "    return (K.abs(y_pred - y_true) / ((K.abs(y_true) + K.abs(y_pred))))*100\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    import keras.backend as K\n",
        "\n",
        "    return (K.abs(y_true - y_pred))\n",
        "def build__model():\n",
        "\n",
        "  # define LSTM model\n",
        "  model = Sequential()\n",
        "\n",
        "  # Input shape: (samples, time, channels, rows, cols) see: https://keras.io/layers/recurrent/#convlstm2d\n",
        "  model.add(ConvLSTM2D(\n",
        "          filters=8,\n",
        "          kernel_size=(10, 10),\n",
        "          input_shape=(7, 1, 9, 1),\n",
        "          padding='same',\n",
        "          return_sequences=True))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(ConvLSTM2D(\n",
        "          filters=8,\n",
        "          kernel_size=(5, 5),\n",
        "          padding='same',\n",
        "          return_sequences=False))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(32, kernel_initializer='TruncatedNormal'))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(16, kernel_initializer='TruncatedNormal'))\n",
        "  model.add(LeakyReLU())\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1))\n",
        "  sgd = SGD(lr=0.00001, momentum=0.9, decay=0, nesterov=True) # sgd in general yields better results, but needs a lot of tweeking and is slower\n",
        "  adam = Adam(lr=lr)\n",
        "\n",
        "  # compile & fit\n",
        "  model.compile(optimizer=adam, loss = ['mse'], metrics=[mape, smape, mae])\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "def pred_result(df):\n",
        "  def mse(targets, predictions):\n",
        "      return ((predictions - targets) ** 2).mean()\n",
        "  def rmse(targets, predictions):\n",
        "      return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "  def rsquared(x, y):\n",
        "      slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
        "      return r_value**2\n",
        "\n",
        "  from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "  def mae(targets, predictions): \n",
        "    return mean_absolute_error(targets.reshape(-1, 1), predictions.reshape(-1, 1))\n",
        "  magnitude = 1\n",
        "  y_preds = model.predict(X_test)\n",
        "  y_true = y_test.reshape(y_test.shape[0], 1)\n",
        "  \n",
        "  split_index = int(df.shape[0]*test_size)\n",
        "  x = df[split_index:]\n",
        "  #len(y_true), len(x)\n",
        "\n",
        "  datetime_difference = len(x) - len(y_true)\n",
        "  x = x[datetime_difference:] # Correct for datetime difference, this is a dirty way of doing it\n",
        "\n",
        "  plt.figure(figsize=(20,10))\n",
        "  plt.plot(x.index, y_true, '.-', color='red', label='Real values', alpha=0.5)\n",
        "  plt.plot(x.index, y_preds, '.-', color='blue', label='Predicted values')\n",
        "\n",
        "  plt.ylabel(r'gasPower $\\cdot$ 10$^{-%s}$ [m$^3$/h]' % magnitude, fontsize=14)\n",
        "  plt.xlabel('datetime [-]', fontsize=14) #TODO: set x values as actual dates\n",
        "\n",
        "  plt.legend(loc='upper left', borderaxespad=0, frameon=False, fontsize=14, markerscale=3)\n",
        "\n",
        "  mse_result = model.evaluate(X_test, y_test)[0]\n",
        "  mape_result = model.evaluate(X_test, y_test)[1]\n",
        "  smape_result = model.evaluate(X_test, y_test)[2]\n",
        "  mae_result = model.evaluate(X_test, y_test)[3]\n",
        "  from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "  rmse = mse_result**(1/2)\n",
        "  mse_result = model.evaluate(X_test, y_test)[0]\n",
        "  mape_result = model.evaluate(X_test, y_test)[1]\n",
        "  smape_result = model.evaluate(X_test, y_test)[2]\n",
        "  mse_result = mse(y_true, y_preds)\n",
        "  #rmse = rmes(y_true, y_preds)\n",
        "  #rsquared = rsquared(y_true, y_preds)\n",
        "\n",
        "\n",
        "  plt.title('ConvLSTM2D result \\n MSE = %.2f \\n RMSE = %.3f \\n MAPE = %.1f [%%] \\n MAE = %.3f [%%] ' % (mse_result, rmse, mape_result, mae_result), fontsize = 14)\n",
        "\n",
        "  #plt.savefig('figures/LSTM result hourly.png', dpi=300)\n",
        "  print('FINISHED')"
      ],
      "metadata": {
        "id": "TRRsdKGJdoXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#결정사항\n",
        "look_back = timestep # D -> 5, H -> 5*24\n",
        "num_features = len(input_list)\n",
        "output_dim = len(output_col)\n",
        "test_size = 0.7\n",
        "hidden_nodes = 35 # 35\n",
        "lr= 1e-4\n",
        "epochs = 5 #135\n",
        "bs = 2**6\n",
        "\n",
        "#모델링\n",
        "X_train, y_train, X_test, y_test = df_to_lstm_format(df=imputed_data, test_size=test_size, look_back=look_back, target_column=output_col, scale_X=True)\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1, X_train.shape[2], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1, X_test.shape[2], 1)\n",
        "\n",
        "early_stopping_monitor = EarlyStopping(patience=5000)\n",
        "model = build__model()\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=bs, validation_data=(X_test, y_test),verbose=1, callbacks=[PlotLossesKeras(), early_stopping_monitor])\n",
        "pred_result(imputed_data)"
      ],
      "metadata": {
        "id": "mArIckLOdqzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
