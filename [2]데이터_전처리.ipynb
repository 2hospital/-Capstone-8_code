{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[2]데이터 전처리",
      "provenance": [],
      "authorship_tag": "ABX9TyO/mElrTlhwcGaj4BydEGC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doyeo-n/-Capstone-8_code/blob/main/%5B2%5D%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#필요한 패키지 설치\n",
        "!pip install fancyimpute\n",
        "!pip install missingpy\n",
        "!pip install impyute\n",
        "!pip install sklearn\n",
        "!pip install missingpy\n",
        "!pip install impyute\n",
        "!pip install sklearn\n",
        "!pip install ujson\n",
        "!pip install utils\n",
        "!pip install ipdb\n",
        "!pip install shap\n",
        "!pip install pykalman\n",
        "!pip install ujson\n",
        "!pip install utils\n",
        "!pip install ipdb\n",
        "!pip install shap\n",
        "!pip install pykalman\n",
        "\n",
        "#torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import shap\n",
        "\n",
        "import numpy as np\n",
        "from numpy import ma\n",
        "import pandas as pd\n",
        "import pickle as pickle\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import ujson as json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import ipdb\n",
        "from ipdb import set_trace\n",
        "import scipy\n",
        "import scipy.stats\n",
        "from pykalman import KalmanFilter\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import seaborn as sb\n",
        "# multivariate multi-step encoder-decoder lstm\n",
        "from math import sqrt\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from impyute.imputation.cs import mice\n",
        "import shap\n",
        "import numpy as np\n",
        "from numpy import ma\n",
        "import pandas as pd\n",
        "import pickle as pickle\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import ujson as json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import ipdb\n",
        "from ipdb import set_trace\n",
        "import scipy\n",
        "import scipy.stats\n",
        "from pykalman import KalmanFilter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "#데이터 불러오기 및 전처리\n",
        "def load_data(path, input_list, index_col, output_col):\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    original = pd.read_csv(path, encoding = \"cp949\", thousands = ',') \n",
        "    original = original[input_list]\n",
        "    original[index_col] = pd.to_datetime(original[index_col])\n",
        "    original = original.set_index(index_col)\n",
        "    output_var = [output_col]\n",
        "    attributes = input_list\n",
        "    #로그 스케일 변환\n",
        "    for attr in attributes[2:-1]:\n",
        "      print(attr)\n",
        "      original[attr] = np.log1p(original[attr])\n",
        "    data = original\n",
        "\n",
        "    #EDA\n",
        "    fig = plt.figure(figsize = (20, 10)) \n",
        "    rows = 3; cols = 5\n",
        "    o = 1\n",
        "    for attr in attributes[1:]:\n",
        "        ax = fig.add_subplot(rows, cols, o)\n",
        "        im= ax.hist(data[attr], bins = 20)\n",
        "        ax.set_title(attr)\n",
        "        o += 1\n",
        "    plt.show()\n",
        "    return data, original\n",
        "\n",
        "def kalman(eutrophication):\n",
        "  data_kalman = eutrophication.copy()\n",
        "  for i in range(1, len(data_kalman.columns)):\n",
        "    kf = KalmanFilter(initial_state_mean = np.nanmean(data_kalman.iloc[:,i]), n_dim_obs = 1)\n",
        "    X_masked = pd.isna(data_kalman.iloc[:,i])\n",
        "    X =data_kalman.iloc[:, i]\n",
        "    X = ma.array(X)\n",
        "    X.mask = X_masked\n",
        "    (smoothed_state_means, smoothed_state_covariances) = kf.em(X, n_iter=5).smooth(X)\n",
        "    data_kalman.iloc[:,i] = np.where(pd.isna(data_kalman.iloc[:, i]) == True, smoothe_state_means.reshape(-1), dta_kalman.iloc[:, i])\n",
        "  return data_kalman\n",
        "\n",
        "\n",
        "\n",
        "def imputation(imput_method):\n",
        "  !pip install impyute\n",
        "  !pip install sklearn\n",
        "  !pip install fancyimpute\n",
        "  from pandas import read_csv\n",
        "  from matplotlib import pyplot\n",
        "  from impyute.imputation.cs import mice\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import datetime\n",
        "  from datetime import datetime\n",
        "\n",
        "  \"\"\"\n",
        "  start_date = data.index[0]\n",
        "  start_date = datetime.now().strftime('%Y-%m-%d')\n",
        "  #start_date = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "  end_date = data.index[-1]\n",
        "  end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "  #end_date = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "  \"\"\"\n",
        "  #print(f' start date : {start_date}, end date : {end_date}')\n",
        "  data, original = load_data(path, input_list, index_col, output_col)\n",
        "  eutrophication = pd.DataFrame(np.array(pd.date_range(start=\"2012-08-27\",end=\"2021-04-26\")))\n",
        "  eutrophication.columns = [index_col]\n",
        "  eutrophication[index_col] = pd.to_datetime(eutrophication[index_col])\n",
        "  eutrophication = pd.merge(left = eutrophication, right = original, on = 'date', how='outer')\n",
        "  for i in range(1, len(eutrophication.columns)):\n",
        "    name_c = eutrophication.columns[i]\n",
        "    eutrophication[name_c] = np.where(eutrophication[name_c]>=0, eutrophication[name_c], np.nan)\n",
        "  print(eutrophication)\n",
        "  if imput_method == 'kalman':\n",
        "    imputed_data = kalman(eutrophication)\n",
        "  elif imput_method == 'mice':\n",
        "    data = eutrophication\n",
        "    data_mice = data.copy()\n",
        "    data_mice = mice((data_mice.iloc[:, 1:].values))\n",
        "    data_mice= pd.DataFrame(data_mice)\n",
        "    print(data_mice)\n",
        "    data_mice.insert(0, 'date', data[index_col])    \n",
        "    data_mice.columns = data.columns\n",
        "    print(data_mice.columns)\n",
        "    imputed_data = data_mice\n",
        "\n",
        "  groups = [0, 1, 2, 3, 5, 6, 7,8,9]\n",
        "  i = 1\n",
        "  # plot each column\n",
        "  pyplot.figure(figsize = (20,15))\n",
        "  for group in groups:\n",
        "    pyplot.subplot(len(groups), 1, i)\n",
        "    pyplot.plot(imputed_data.iloc[:, group])\n",
        "    pyplot.title(imputed_data.columns[group], y=0.5, loc='right')\n",
        "    i += 1\n",
        "  pyplot.show()\n",
        "  return eutrophication, imputed_data"
      ],
      "metadata": {
        "id": "ZHb_DPW06xT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#데이터 불러오기 및 전처리\n",
        "def load_data(path, input_list, index_col, output_col):\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    original = pd.read_csv(path, encoding = \"cp949\", thousands = ',') \n",
        "    original = original[input_list]\n",
        "    original[index_col] = pd.to_datetime(original[index_col])\n",
        "    original = original.set_index(index_col)\n",
        "    output_var = [output_col]\n",
        "    attributes = input_list\n",
        "    #로그 스케일 변환\n",
        "    for attr in attributes[2:-1]:\n",
        "      print(attr)\n",
        "      original[attr] = np.log1p(original[attr])\n",
        "    data = original\n",
        "\n",
        "    #EDA\n",
        "    fig = plt.figure(figsize = (20, 10)) \n",
        "    rows = 3; cols = 5\n",
        "    o = 1\n",
        "    for attr in attributes[1:]:\n",
        "        ax = fig.add_subplot(rows, cols, o)\n",
        "        im= ax.hist(data[attr], bins = 20)\n",
        "        ax.set_title(attr)\n",
        "        o += 1\n",
        "    plt.show()\n",
        "    return data, original\n",
        "\n",
        "def imputation(data, original):\n",
        "  !pip install impyute\n",
        "  !pip install sklearn\n",
        "  !pip install fancyimpute\n",
        "  from pandas import read_csv\n",
        "  from matplotlib import pyplot\n",
        "  from impyute.imputation.cs import mice\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import datetime\n",
        "  from datetime import datetime\n",
        "\n",
        "  \"\"\"\n",
        "  start_date = data.index[0]\n",
        "  start_date = datetime.now().strftime('%Y-%m-%d')\n",
        "  #start_date = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
        "  end_date = data.index[-1]\n",
        "  end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "  #end_date = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
        "  \"\"\"\n",
        "  #print(f' start date : {start_date}, end date : {end_date}')\n",
        "  data, original = load_data(path, input_list, index_col, output_col)\n",
        "  eutrophication = pd.DataFrame(np.array(pd.date_range(start=\"2012-08-27\",end=\"2021-04-26\")))\n",
        "  eutrophication.columns = [index_col]\n",
        "  eutrophication[index_col] = pd.to_datetime(eutrophication[index_col])\n",
        "  eutrophication = pd.merge(left = eutrophication, right = original, on = 'date', how='outer')\n",
        "\n",
        "  for i in range(1, len(eutrophication.columns)):\n",
        "    name_c = eutrophication.columns[i]\n",
        "    eutrophication[name_c] = np.where(eutrophication[name_c]>=0, eutrophication[name_c], np.nan)\n",
        "  print(eutrophication)\n",
        "  return eutrophication\n",
        "\n",
        "def rea_imp(eutrophication, imput_method):\n",
        "  if imput_method == 'mice':\n",
        "    data = eutrophication\n",
        "    data_mice = data.copy()\n",
        "    data_mice = mice((data_mice.iloc[:, 1:].values))\n",
        "    data_mice= pd.DataFrame(data_mice)\n",
        "    print(data_mice)\n",
        "    data_mice.insert(0, 'date', data[index_col])    \n",
        "    data_mice.columns = data.columns\n",
        "    print(data_mice.columns)\n",
        "    imputed_data = data_mice\n",
        "  groups = [0, 1, 2, 3, 5, 6, 7,8,9]\n",
        "  i = 1\n",
        "  # plot each column\n",
        "  pyplot.figure(figsize = (20,15))\n",
        "  for group in groups:\n",
        "    pyplot.subplot(len(groups), 1, i)\n",
        "    pyplot.plot(imputed_data.iloc[:, group])\n",
        "    pyplot.title(imputed_data.columns[group], y=0.5, loc='right')\n",
        "    i += 1\n",
        "  pyplot.show()\n",
        "  return imputed_data"
      ],
      "metadata": {
        "id": "317vlfF46x22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i826xr0i6vOb"
      },
      "outputs": [],
      "source": [
        "#결정사항\n",
        "input_list = ['date', 'temp_WQ', 'irr', 'RT', 'prec_2', 'DO_WQ', 'BOD_WQ', 'TN_WQ', 'TP_WQ', 'EColi_WQ', 'log_cyan']\n",
        "output_col = 'log_cyan'\n",
        "index_col = 'date'\n",
        "path = './gdrive/My Drive/Colab Notebooks/chang.csv'\n",
        "n_input = 7\n",
        "#mice 또는 kalman 입력\n",
        "imput_method = 'kalman' \n",
        "\n",
        "\n",
        "#데이터 전처리\n",
        "original, data = load_data(path,input_list, index_col, output_col)\n",
        "eutrophication = imputation(original, data)\n",
        "data_kalman = data.copy()\n",
        "for i in range(1, len(data_kalman.columns)-1):\n",
        " kf = KalmanFilter(initial_state_mean=np.nanmean(data_kalman.iloc[:, i]), n_dim_obs=1)\n",
        " X_masked = pd.isna(data_kalman.iloc[:, i])\n",
        " X = data_kalman.iloc[:, i]\n",
        " X = ma.array(X)\n",
        " X.mask = X_masked\n",
        " (smoothed_state_means, smoothed_state_covariances) = kf.em(X, n_iter=5).smooth(X)\n",
        " data_kalman.iloc[:, i] = np.where(pd.isna(data_kalman.iloc[:, i]) == True, smoothed_state_means.reshape(-1), data_kalman.iloc[:, i])\n",
        "imputed_data = data_kalman\n",
        "\n",
        "if imput_method == 'mice':\n",
        "  imputed_data = rea_imp(eutrophication, imput_method)"
      ]
    }
  ]
}